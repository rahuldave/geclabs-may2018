{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "hide": true
      },
      "source": [
        "# Validation and Regularization\n",
        "\n",
        "![](images/hair.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "hide": true
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "pd.set_option('display.width', 500)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.notebook_repr_html', True)\n",
        "import seaborn.apionly as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"poster\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "hide": true
      },
      "outputs": [],
      "source": [
        "def make_simple_plot():\n",
        "    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n",
        "    axes[0].set_ylabel(\"$y$\")\n",
        "    axes[0].set_xlabel(\"$x$\")\n",
        "    axes[1].set_xlabel(\"$x$\")\n",
        "    axes[1].set_yticklabels([])\n",
        "    axes[0].set_ylim([-2,2])\n",
        "    axes[1].set_ylim([-2,2])\n",
        "    plt.tight_layout();\n",
        "    return axes\n",
        "def make_plot():\n",
        "    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n",
        "    axes[0].set_ylabel(\"$p_R$\")\n",
        "    axes[0].set_xlabel(\"$x$\")\n",
        "    axes[1].set_xlabel(\"$x$\")\n",
        "    axes[1].set_yticklabels([])\n",
        "    axes[0].set_ylim([0,1])\n",
        "    axes[1].set_ylim([0,1])\n",
        "    axes[0].set_xlim([0,1])\n",
        "    axes[1].set_xlim([0,1])\n",
        "    plt.tight_layout();\n",
        "    return axes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PART 1: Reading in and sampling from the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"data/noisypopulation.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "x=df.f.values\n",
        "f=df.x.values\n",
        "y = df.y.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From 200 points on this curve, we'll make a random choice of 120 points. We do it by choosing the indexes randomly, and then using these indexes as a way of grtting the appropriate samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "indexes=np.sort(np.random.choice(x.shape[0], size=120, replace=False))\n",
        "indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "samplex = x[indexes]\n",
        "samplef = f[indexes]\n",
        "sampley = y[indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "figure_type": "m"
      },
      "outputs": [],
      "source": [
        "plt.plot(x,f, 'k-', alpha=0.6, label=\"f\");\n",
        "plt.plot(x[indexes], y[indexes], 's', alpha=0.3, ms=10, label=\"in-sample y (observed)\");\n",
        "plt.plot(x, y, '.', alpha=0.8, label=\"population y\");\n",
        "plt.xlabel('$x$');\n",
        "plt.ylabel('$y$')\n",
        "plt.legend(loc=4);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "sample_df=pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Fit on training set and predict on test set\n",
        "\n",
        "We will do the split of testing and training for you in order to illustrate how this can be done.\n",
        "\n",
        "### Train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "datasize=sample_df.shape[0]\n",
        "print(datasize)\n",
        "#split dataset using the index, as we have x,f, and y that we want to split.\n",
        "itrain,itest = train_test_split(np.arange(120),train_size=0.8)\n",
        "print(itrain.shape)\n",
        "xtrain= sample_df.x[itrain].values\n",
        "ftrain = sample_df.f[itrain].values\n",
        "ytrain = sample_df.y[itrain].values\n",
        "xtest= sample_df.x[itest].values\n",
        "ftest = sample_df.f[itest].values\n",
        "ytest = sample_df.y[itest].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Polynomial features\n",
        "\n",
        "We'll write a function to encapsulate what we learnt about creating the polynomial features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def make_features(train_set, test_set, degrees):\n",
        "    train_dict = {}\n",
        "    test_dict = {}\n",
        "    for d in degrees:\n",
        "        traintestdict={}\n",
        "        train_dict[d] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n",
        "        test_dict[d] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n",
        "    return train_dict, test_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. You should worry that a given split exposes us to the peculiarity of the data set that got randomly chosen for us. This naturally leads us to want to choose multiple such random splits and somehow average over this process to find the \"best\" validation minimizing polynomial degree or complexity $d$.\n",
        "2. The multiple splits process also allows us to get an estimate of how consistent our prediction error is: in other words, just like in the hair example, it gives us a distribution.\n",
        "3. Furthermore the validation set that we left out has two competing demands on it. The larger the set is, the better is our estimate of the out-of-sample error. So we'd like to hold out as much as possible. But the smaller the validation set is, the more data we have to train our model on. This allows us to have more smaller sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The idea is illustrated in the figure below, for a given hypothesis set $\\cal{H}_a$ with complexity parameter $d=a$ (the polynomial degree). We do the train/validate split, not once but multiple times. \n",
        "\n",
        "In the figure below we create 4-folds from the training set part of our data set $\\cal{D}$. By this we mean that we divide our set roughly into 4 equal parts. As illustrated below, this can be done in 4 different ways, or folds. In each fold we train a model on 3 of the parts. The model so trained is denoted as $g^-_{Fi}$, for example $g^-_{F3}$ . The minus sign in the superscript once again indicates that we are training on a reduced set. The $F3$ indicates that this model was trained on the third fold. Note that the model trained on each fold will be different!\n",
        "\n",
        "For each fold, after training the model, we calculate the risk or error on the remaining one validation part. We then add the validation errors together from the different folds, and divide by the number of folds to calculate an average error. Note again that this average error is an average over different models $g^-_{Fi}$. We use this error as the validation error for $d=a$ in the validation process described earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![m:caption](images/train-cv2.png)\n",
        "\n",
        "Note that the number of folds is equal to the number of splits in the data. For example, if we have 5 splits, there will be 5 folds. To illustrate cross-validation consider below fits in $\\cal{H}_0$ and $\\cal{H}_1$ (means and straight lines) to a sine curve, with only 3 data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The entire description of K-fold Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We put thogether this scheme to calculate the error for a given polynomial degree $d$ with the method we used earlier to choose a model given the validation-set risk as a function of $d$:\n",
        "\n",
        "1. create `n_folds` partitions of the training data. \n",
        "2. We then train on `n_folds -1` of these partitions, and test on the remaining partition. There are `n_folds` such combinations of partitions (or folds), and thus we obtain `n_fold` risks.\n",
        "3. We average the error or risk of all such combinations to obtain, for each value of $d$, $R_{dCV}$.\n",
        "4. We move on to the next value of $d$, and repeat 3\n",
        "5. and then find the optimal value of d that minimizes risk $d=*$.\n",
        "5. We finally use that value to make the final fit in $\\cal{H}_*$ on the entire old training set.\n",
        "\n",
        "![caption](images/train-cv3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can also shown that **cross-validation error is an unbiased estimate of the out of sample-error**.\n",
        "\n",
        "Let us now do 4-fold cross-validation on our  data set. We increase the complexity from degree 0 to degree 20. In each case we take the old training set, split in 4 ways into 4 folds, train on 3 folds, and calculate the validation error on the remaining one. We then average the erros over the four folds to get a cross-validation error for that $d$. Then we did what we did before: find the hypothesis space $\\cal{H}_*$ with the lowest cross-validation error, and refit it using the entire training set. We can then use the test set to estimate $E_{out}$.\n",
        "\n",
        "We will use `KFold` from `scikit-learn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "n_folds=4\n",
        "kfold = KFold(n_folds)\n",
        "list(kfold.split(range(48)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is wrong with the above? Why must we do the below?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "kfold = KFold(n_folds, shuffle=True)\n",
        "list(kfold.split(range(48)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "for train, valid in kfold.split(range(48)):\n",
        "    print(\"TRAIN INDICES\", train)\n",
        "    print(\"VALID INDICES\", valid)\n",
        "    print(\"VALID VALUES\", xtrain[valid])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4-fold CV on our data set\n",
        "\n",
        ">YOUR TURN HERE: Carry out 4-Fold validation. For each fold, you will need to first create the polynomial features. for each degree polynomial, fit on the smaller training set and predict on the validation set. Store the MSEs, for each degree and each fold, in `train_errors` and `valid_errors`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "n_folds=4\n",
        "degrees=range(21)\n",
        "train_errors = np.zeros((21,4))\n",
        "valid_errors = np.zeros((21,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We average the MSEs over the folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "mean_train_errors = train_errors.mean(axis=1)\n",
        "mean_valid_errors = valid_errors.mean(axis=1)\n",
        "std_train_errors = train_errors.std(axis=1)\n",
        "std_valid_errors = valid_errors.std(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We find the degree that minimizes the `cross-validation` error, and just like before, refit the model on the entire training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "mindeg = np.argmin(mean_valid_errors)\n",
        "print(mindeg)\n",
        "post_cv_train_dict, test_dict=make_features(xtrain, xtest, degrees)\n",
        "#fit on whole training set now.\n",
        "est = LinearRegression()\n",
        "est.fit(post_cv_train_dict[mindeg], ytrain) # fit\n",
        "pred = est.predict(test_dict[mindeg])\n",
        "err = mean_squared_error(pred, ytest)\n",
        "errtr=mean_squared_error(ytrain, est.predict(post_cv_train_dict[mindeg]))\n",
        "c0=sns.color_palette()[0]\n",
        "c1=sns.color_palette()[1]\n",
        "with sns.plotting_context('poster'):\n",
        "    plt.plot(degrees, mean_train_errors, marker='o', label='error train', alpha=0.9)\n",
        "    plt.plot(degrees, mean_valid_errors, marker='o', label='CV error', alpha=0.9)\n",
        "\n",
        "\n",
        "    plt.fill_between(degrees, mean_valid_errors-std_valid_errors, mean_valid_errors+std_valid_errors, color=c1, alpha=0.2)\n",
        "\n",
        "\n",
        "    plt.plot([mindeg], [err], 'o',  label='test set error')\n",
        "\n",
        "    plt.ylabel('mean squared error')\n",
        "    plt.xlabel('degree')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.yscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the cross-validation error minimizes at a low degree, and then increases. Because we have so few data points the spread in fold errors increases as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upto now we have focussed on finding the polynomial with the right degree of complecity $d=*$ given the data that we have.\n",
        "\n",
        "When we **regularize** we smooth or restrict the choices of the kinds of 20th order polynomials that we allow in our fits. \n",
        "\n",
        "That is, if we want to fit with a 20th order polynomial, ok, lets fit with it, but lets reduce the size of, or limit the functions in $\\cal{H}_{20}$ that we allow.\n",
        "\n",
        "\n",
        "We do this by a **soft constraint** by setting:\n",
        "\n",
        "$$\\sum_{i=0}^j a_i^2 < C.$$\n",
        "\n",
        "This setting is called the Ridge.\n",
        "\n",
        "This ensures that the coefficients dont get too high, which makes sure we dont get wildly behaving pilynomials with high coefficients. \n",
        "\n",
        "It turns out that we can do this by adding a term to the risk that we minimize on the training data for $\\cal{H}_j$ (seeing why is beyond the scope here but google on lagrange multipliers and the dual problem):\n",
        "\n",
        "$$\\cal{R}(h_j) =  \\sum_{y_i \\in \\cal{D}} (y_i - h_j(x_i))^2 +\\alpha \\sum_{i=0}^j a_i^2.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regularization of our model with Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def plot_functions(d, est, ax, df, alpha, xtest, Xtest, xtrain, ytrain):\n",
        "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n",
        "    ax.plot(df.x, df.f, color='k', label='f')\n",
        "    ax.plot(xtrain, ytrain, 's', label=\"training\", ms=5, alpha=0.3)\n",
        "    ax.plot(xtest, ytest, 's', label=\"testing\", ms=5, alpha=0.3)\n",
        "    transx=np.arange(0,1.1,0.01)\n",
        "    transX = PolynomialFeatures(d).fit_transform(transx.reshape(-1,1))\n",
        "    ax.plot(transx, est.predict(transX),  '.', ms=7, alpha=0.8, label=\"alpha = %s\" % str(alpha))\n",
        "    ax.set_ylim((0, 1))\n",
        "    ax.set_xlim((0, 1))\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_xlabel('x')\n",
        "    ax.legend(loc='lower right')\n",
        "    \n",
        "def plot_coefficients(est, ax, alpha):\n",
        "    coef = est.coef_.ravel()\n",
        "    ax.semilogy(np.abs(coef), marker='o', label=\"alpha = %s\" % str(alpha))\n",
        "    ax.set_ylim((1e-1, 1e15))\n",
        "    ax.set_ylabel('abs(coefficient)')\n",
        "    ax.set_xlabel('coefficients')\n",
        "    ax.legend(loc='upper left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The addition of a penalty term to the risk or error causes us to choose a smaller subset of the entire set of complex $\\cal{H}_{20}$ polynomials. This is shown in the diagram below where the balance between bias and variance occurs at some subset $S_*$ of the set of 20th order polynomials indexed by $\\alpha_*$ (there is an error on the diagram, the 13 there should actually be a 20).\n",
        "\n",
        "![m:caption](images/complexity-error-reg.png)\n",
        "\n",
        "Lets see what some of the $\\alpha$s do. The diagram below trains on the entire training set, for given values of $\\alpha$, minimizing the penalty-term-added training error.\n",
        "\n",
        "**Note that here we are doing the note so good thing of exhausting the test set for demonstration purposes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "fig, rows = plt.subplots(6, 2, figsize=(12, 24))\n",
        "d=20\n",
        "alphas = [0.0, 1e-6, 1e-5, 1e-3, 0.01, 1]\n",
        "train_dict, test_dict = make_features(xtrain, xtest, degrees)\n",
        "Xtrain = train_dict[d]\n",
        "Xtest = test_dict[d]\n",
        "for i, alpha in enumerate(alphas):\n",
        "    l,r=rows[i]\n",
        "    est = Ridge(alpha=alpha)\n",
        "    est.fit(Xtrain, ytrain)\n",
        "    plot_functions(d, est, l, df, alpha, xtest, Xtest, xtrain, ytrain )\n",
        "    plot_coefficients(est, r, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, as we increase $\\alpha$ from 0 to 1, we start out overfitting, then doing well, and then, our fits, develop a mind of their own irrespective of data, as the penalty term dominates the risk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `GridSearchCV` meta-estimator\n",
        "\n",
        "Lets use cross-validation to figure what this critical $\\alpha_*$ is. To do this we use the concept of a *meta-estimator* from scikit-learn. As the API paper puts it:\n",
        "\n",
        ">In scikit-learn, model selection is supported in two distinct meta-estimators, GridSearchCV and RandomizedSearchCV. They take as input an estimator (basic or composite), whose hyper-parameters must be optimized, and a set of hyperparameter settings to search through.\n",
        "\n",
        "The concept of a meta-estimator allows us to wrap, for example, cross-validation, or methods that build and combine simpler models or schemes. For example:\n",
        "\n",
        "    est = Ridge()\n",
        "    parameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n",
        "    gridclassifier=GridSearchCV(est, param_grid=parameters, cv=4, scoring=\"mean_squared_error\")\n",
        "    \n",
        "The `GridSearchCV` replaces the manual iteration over thefolds using `KFolds` and the averaging we did previously, doing it all for us. It takes a parameter grid in the shape of a dictionary as input, and sets $\\alpha$ to the appropriate parameter values one by one. It then trains the model, cross-validation fashion, and gets the error. Finally it compares the errors for the different $\\alpha$'s, and picks the best choice model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "def cv_optimize_ridge(X, y, n_folds=4):\n",
        "    est = Ridge()\n",
        "    parameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n",
        "    #the scoring parameter below is the default one in ridge, but you can use a different one\n",
        "    #in the cross-validation phase if you want.\n",
        "    gs = GridSearchCV(est, param_grid=parameters, cv=n_folds, scoring=\"neg_mean_squared_error\")\n",
        "    gs.fit(X, y)\n",
        "    return gs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "fitmodel = cv_optimize_ridge(Xtrain, ytrain, n_folds=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "fitmodel.cv_results_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our best model occurs for $\\alpha=0.01$. We also output the mean cross-validation error at different $\\alpha$ (with a negative sign, as scikit-learn likes to maximize negative error which is equivalent to minimizing error).\n",
        "\n",
        "### Refitting on entire training set\n",
        "\n",
        "We refit the estimator on old training set, and calculate and plot the test set error and the polynomial coefficients. Notice how many of these coefficients have been pushed to lower values or 0.\n",
        "\n",
        "\n",
        ">YOUR TURN NOW: assign to variable est the classifier obtained by fitting the entire training set using the best $\\alpha$ found above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Store in est a new classifier fit on the entire training set with the best alpha\n",
        "#your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def plot_functions_onall(est, ax, df, alpha, xtrain, ytrain, Xtrain, xtest, ytest):\n",
        "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n",
        "    ax.plot(df.x, df.f, color='k', label='f')\n",
        "    ax.plot(xtrain, ytrain, 's', alpha=0.4, label=\"train\")\n",
        "    ax.plot(xtest, ytest, 's', alpha=0.6, label=\"test\")\n",
        "    transx=np.arange(0,1.1,0.01)\n",
        "    transX = PolynomialFeatures(20).fit_transform(transx.reshape(-1,1))\n",
        "    ax.plot(transx, est.predict(transX), '.', alpha=0.6, label=\"alpha = %s\" % str(alpha))\n",
        "    #print est.predict(transX)\n",
        "    ax.set_ylim((0, 1))\n",
        "    ax.set_xlim((0, 1))\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_xlabel('x')\n",
        "    ax.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, rows = plt.subplots(1, 2, figsize=(12, 5))\n",
        "l,r=rows\n",
        "plot_functions_onall(est, l, df, alphawechoose, xtrain, ytrain, Xtrain, xtest, ytest)\n",
        "plot_coefficients(est, r, alphawechoose)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the best fit model is now chosen from the entire set of 20th order polynomials, and a non-zero hyperparameter $\\alpha$ that we fit for ensures that only smooth models amonst these polynomials are chosen, by setting most of the polynomial coefficients to something close to 0 (Lasson sets them exactly to 0).\n",
        "\n",
        "\n",
        "> YOUR TURN NOW\n",
        "\n",
        ">Try 2 experiments. Repeat the whole notebook with 60 chosen points. Why do we seem to need a larger alpha than before? In the non-regularized part, why id the estimated d smaller? Finally, compare the $\\alpha$ obtained on the validation set against the informal $\\alpha$ we obtained on the test set a bit above..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}